
/*
  Generated by gemm_operation.py - Do not edit.
*/

///////////////////////////////////////////////////////////////////////////////////////////////////

#include "cutlass/cutlass.h"
#include "cutlass/library/library.h"
#include "cutlass/library/manifest.h"
#include "library_internal.h"
#include "gemm_operation.h"
#include "gemm_operation_3x.hpp"
#include "grouped_gemm_operation_3x.hpp"
#include "sparse_gemm_operation_3x.hpp"
#include "block_scaled_gemm_operation_3x.hpp"
#include "blockwise_gemm_operation_3x.hpp"
#include "cutlass/arch/wmma.h"
#include "cutlass/numeric_types.h"
#include "cutlass/gemm/gemm.h"
#include "cutlass/gemm/kernel/gemm_universal.hpp"
#include "cutlass/gemm/collective/collective_builder.hpp"
#include "cutlass/epilogue/collective/collective_builder.hpp"
#include "cutlass/detail/blockwise_scale_layout.hpp"

///////////////////////////////////////////////////////////////////////////////////////////////////



using cutlass3x_sm100_tensorop_gemm_grouped_64x128f32xe4m3_256x128f32xe5m2_f32_void_bf16_64x256x128_0x0x1_0_tnn_align16_1sm_epi_nosmem_epilogue =
  typename cutlass::epilogue::collective::CollectiveBuilder<
    cutlass::arch::Sm100, cutlass::arch::OpClassTensorOp,
    cute::Shape<cute::_64, cute::_256, cute::_128>,
    cute::Shape<int, int, cute::_1>,
    cutlass::epilogue::collective::EpilogueTileAuto,
    float, float,
    void, cutlass::layout::ColumnMajor* , 8,
    cutlass::bfloat16_t, cutlass::layout::ColumnMajor* , 8,
    cutlass::epilogue::PtrArrayBlockwiseNoSmemWarpSpecialized1Sm,
    cutlass::epilogue::fusion::LinearCombination<
      cutlass::bfloat16_t,
      float,
      void,
      float
    >
  >::CollectiveOp;



using cutlass3x_sm100_tensorop_gemm_grouped_64x128f32xe4m3_256x128f32xe5m2_f32_void_bf16_64x256x128_0x0x1_0_tnn_align16_1sm_epi_nosmem_ScaleConfig = cutlass::detail::Sm1xxBlockwiseScaleConfig<64, 256, 128>;
using cutlass3x_sm100_tensorop_gemm_grouped_64x128f32xe4m3_256x128f32xe5m2_f32_void_bf16_64x256x128_0x0x1_0_tnn_align16_1sm_epi_nosmem_LayoutSFA = decltype(cutlass3x_sm100_tensorop_gemm_grouped_64x128f32xe4m3_256x128f32xe5m2_f32_void_bf16_64x256x128_0x0x1_0_tnn_align16_1sm_epi_nosmem_ScaleConfig::deduce_layoutSFA());
using cutlass3x_sm100_tensorop_gemm_grouped_64x128f32xe4m3_256x128f32xe5m2_f32_void_bf16_64x256x128_0x0x1_0_tnn_align16_1sm_epi_nosmem_LayoutSFB = decltype(cutlass3x_sm100_tensorop_gemm_grouped_64x128f32xe4m3_256x128f32xe5m2_f32_void_bf16_64x256x128_0x0x1_0_tnn_align16_1sm_epi_nosmem_ScaleConfig::deduce_layoutSFB());
      

using cutlass3x_sm100_tensorop_gemm_grouped_64x128f32xe4m3_256x128f32xe5m2_f32_void_bf16_64x256x128_0x0x1_0_tnn_align16_1sm_epi_nosmem_mainloop =
  typename cutlass::gemm::collective::CollectiveBuilder<
    cutlass::arch::Sm100, cutlass::arch::OpClassTensorOp,
    cutlass::float_e4m3_t, cute::tuple<cutlass::layout::RowMajor* , cutlass3x_sm100_tensorop_gemm_grouped_64x128f32xe4m3_256x128f32xe5m2_f32_void_bf16_64x256x128_0x0x1_0_tnn_align16_1sm_epi_nosmem_LayoutSFA* >, 16,
    cutlass::float_e5m2_t, cute::tuple<cutlass::layout::ColumnMajor* , cutlass3x_sm100_tensorop_gemm_grouped_64x128f32xe4m3_256x128f32xe5m2_f32_void_bf16_64x256x128_0x0x1_0_tnn_align16_1sm_epi_nosmem_LayoutSFB* >, 16,
    float,
    cute::Shape<cute::_64, cute::_256, cute::_128>,
    cute::Shape<int, int, cute::_1>,
    cutlass::gemm::collective::StageCountAutoCarveout<static_cast<int>(sizeof(typename cutlass3x_sm100_tensorop_gemm_grouped_64x128f32xe4m3_256x128f32xe5m2_f32_void_bf16_64x256x128_0x0x1_0_tnn_align16_1sm_epi_nosmem_epilogue::SharedStorage))>,
    cutlass::gemm::KernelPtrArrayTmaWarpSpecializedBlockwise1SmSm100
  >::CollectiveOp;

// Gemm operator cutlass3x_sm100_tensorop_gemm_grouped_64x128f32xe4m3_256x128f32xe5m2_f32_void_bf16_64x256x128_0x0x1_0_tnn_align16_1sm_epi_nosmem
using cutlass3x_sm100_tensorop_gemm_grouped_64x128f32xe4m3_256x128f32xe5m2_f32_void_bf16_64x256x128_0x0x1_0_tnn_align16_1sm_epi_nosmem_base = cutlass::gemm::kernel::GemmUniversal<
    cutlass::gemm::GroupProblemShape<cute::Shape<int,int,int>>,
    cutlass3x_sm100_tensorop_gemm_grouped_64x128f32xe4m3_256x128f32xe5m2_f32_void_bf16_64x256x128_0x0x1_0_tnn_align16_1sm_epi_nosmem_mainloop,
    cutlass3x_sm100_tensorop_gemm_grouped_64x128f32xe4m3_256x128f32xe5m2_f32_void_bf16_64x256x128_0x0x1_0_tnn_align16_1sm_epi_nosmem_epilogue,
    void>;

// Define named type
struct cutlass3x_sm100_tensorop_gemm_grouped_64x128f32xe4m3_256x128f32xe5m2_f32_void_bf16_64x256x128_0x0x1_0_tnn_align16_1sm_epi_nosmem :
  public cutlass3x_sm100_tensorop_gemm_grouped_64x128f32xe4m3_256x128f32xe5m2_f32_void_bf16_64x256x128_0x0x1_0_tnn_align16_1sm_epi_nosmem_base { };



///////////////////////////////////////////////////////////////////////////////////////////////////

namespace cutlass {
namespace library {

///////////////////////////////////////////////////////////////////////////////////////////////////

void initialize_cutlass3x_sm100_tensorop_gemm_grouped_64x128f32xe4m3_256x128f32xe5m2_f32_void_bf16_64x256x128_0x0x1_0_tnn_align16_1sm_epi_nosmem(Manifest &manifest) {



  {
    using GemmKernel = cutlass::gemm::device::GemmUniversalAdapter<cutlass3x_sm100_tensorop_gemm_grouped_64x128f32xe4m3_256x128f32xe5m2_f32_void_bf16_64x256x128_0x0x1_0_tnn_align16_1sm_epi_nosmem>;
    manifest.append(
      new GroupedBlockwiseGemmUniversal3xOperation<GemmKernel>("cutlass3x_sm100_tensorop_gemm_grouped_64x128f32xe4m3_256x128f32xe5m2_f32_void_bf16_64x256x128_0x0x1_0_tnn_align16_1sm_epi_nosmem"));
  }



}

///////////////////////////////////////////////////////////////////////////////////////////////////

} // namespace library
} // namespace cutlass

///////////////////////////////////////////////////////////////////////////////////////////////////

