//
// Custom activation function kernel for neural networks
// Implements a specialized activation function with optimizations
//

.version 6.0
.target sm_70
.address_size 64

.visible .entry custom_activation(
    .param .u64 input_ptr,
    .param .u64 output_ptr,
    .param .u32 n
) {
    .reg .u32 %tid;
    .reg .u32 %n_val;
    .reg .u64 %input_addr, %output_addr;
    .reg .u64 %input_idx_addr, %output_idx_addr;
    .reg .pred %cond, %positive_cond;
    .reg .f32 %value, %result;
    .reg .f32 %abs_value, %exp_value;
    
    // Get thread ID
    mov.u32 %tid, %tid.x;
    
    // Load parameters
    ld.param.u32 %n_val, [n];
    ld.param.u64 %input_addr, [input_ptr];
    ld.param.u64 %output_addr, [output_ptr];
    
    // Calculate address for this thread
    mul.wide.u32 %input_idx_addr, %tid, 4;  // 4 bytes per float
    add.u64 %input_idx_addr, %input_addr, %input_idx_addr;
    add.u64 %output_idx_addr, %output_addr, %input_idx_addr;
    
    // Bounds check
    setp.lt.u32 %cond, %tid, %n_val;
    
    // Load value from input
    @%cond ld.global.f32 %value, [%input_idx_addr];
    
    // Implement a custom activation function: Swish (x * sigmoid(x))
    // Sigmoid approximation: 0.5 * (1 + tanh(0.5 * x))
    mul.rn.f32 %abs_value, %value, 0.5;
    abs.f32 %abs_value, %abs_value;
    
    // Approximate exponential for negative values
    neg.f32 %exp_value, %abs_value;
    ex2.approx.f32 %exp_value, %exp_value;  // Base-2 exponential approximation
    
    // Calculate sigmoid approximation
    add.rn.f32 %exp_value, %exp_value, 1.0;
    div.rn.f32 %exp_value, 1.0, %exp_value;
    
    // For positive values, use 1 - exp(-x) / (1 + exp(-x))
    setp.gt.f32 %positive_cond, %value, 0.0;
    @%positive_cond sub.rn.f32 %exp_value, 1.0, %exp_value;
    
    // Multiply input by sigmoid (Swish function)
    mul.rn.f32 %result, %value, %exp_value;
    
    // Store result
    @%cond st.global.f32 [%output_idx_addr], %result;
    
    ret;
}